import matplotlib.pyplot as plt
import matplotlib.lines as lines
import matplotlib.dates as mdates
import _pickle as pickle
import numpy as np
import cv2
import json
import io
import os
import torch
import requests
from datetime import datetime

from scipy.stats.kde import gaussian_kde
from scipy.stats import norm
import matplotlib.gridspec as grid_spec


from sklearn.linear_model import LinearRegression

import colorsys
"""
The basic idea is to create a grid of metric results, with each row corresponding to
a collection and each column corresponding to a metric readout. Each individual 
row,column combo will be generated by a function that takes in the metric set
and returns a figure. Then the overall figure can just plot these as a subplot
Perhaps one additional row should be added with an explanation (as text) of each plot
    PLOT 0 - Summary Spiderplot    
    PLOT 1 - MOT metrics as bar chart, with bar colored in red-green range, and label written on bar (horizontal)
    PLOT 2 - State Errors as ridgeline plot with axis in feet and other statistics shown
    PLOT 3 - Confusion Matrix
    PLOT 4 - Histogram of any of the metrics for which we record all values (vx, ax, theta, x_travelled, y_travelled)
    PLOT 5 - bar chart of any other unsupervised metrics 
"""

"""
There should also be a supplementary file with, for each metric, a description, and a best and worst value (for coloration)
"""

def random_pallette(length):
    sat = 0.2
    var = 0.8
    
    # c_base = np.array([[200,100,100],
    #                    [100,200,100],
    #                    [0,0,255]])
    colors = []
    start_hue =  np.random.rand()

    for i in range(length):
        hue = (start_hue + (length//3)*i/length)%1
        color =    colorsys.hsv_to_rgb(hue,sat,var)
        color = np.array(color) * 255

        colors.append(color)
    return np.stack(colors)

#%% Globals 
global results_dir
results_dir = "/data/"

global dpi
dpi = 162
global scale 
scale = 50
#plt.style.use("bmh")
global MD
MD = {
    "precision":{"text":"Precision","best":1,"bad":0.5},
    "recall":{"text":"Recall","best":1,"bad":0.5},
    "true_negative_rate":{"text":"TNR","best":1,"bad":0},
    "pred_match":{"text":"Pred Match Rate", "best":1,"bad":0.5},
    "gt_match":{"text":"GT Match Rate","best":1,"bad":0.5},
    "motp":{"text":"MOTP","best":1,"bad":0.5},
    "mota":{"text":"MOTA","best":1,"bad":0.5},
    "idr":{"text":"ID-Recall","best":1,"bad":0.5},
    "idp":{"text":"ID-Precision","best":1,"bad":0.5},
    "idf1":{"text":"ID-F1","best":1,"bad":0.5},
    "avg_ax_raw":{"text":"Average Acceleration per traj"},
    "avg_vx_raw":{"text":"Average Speed per traj"},
    "vx_raw":{"text":"Velocity"},
    "ax_raw":{"text":"Acceleration"},
    "x_traveled_raw":{"text":"Trajectory Length","best":2000,"bad":300},
    "x_traveled_avg":{"text":"Trajectory Length","best":2000,"bad":300},
    "bps":{"text":"Hz","best":30,"bad":1},
    "traj_count":{"text":"Trajectories","best":np.nan},
    "x_variation":{"text":"X Variation","best":1,"bad":1.2},
    "y_variation":{"text":"Y Variation","best":0,"bad":50},
    "duration_avg":{"text": "Duration", "best":np.nan},
    "percent_backwards":{"text":"Backwards %","best":0,"bad":1},
    "overlaps_per_object":{"text":"Overlaps/Traj","best":0,"bad":1},
    "mostly_tracked%":{"text":">80% Tracked","best":1,"bad":0.2},
    "mostly_lost%":{"text":"<20% Tracked","best":0,"bad":0.4},
    "num_switches_per_gt":{"text":"Switches per GT","best":0,"bad":2},
    "num_fragmentations_per_gt":{"text":"Frag per GT","best":0,"bad":2},
    "pred_avg_matches":{"text":"GT IDs / Pred","best":1,"bad":2},
    "gt_avg_matches":{"text":"Pred IDs / GT","best":1,"bad":2},
    "per_gt_recall":{"text":"Recall / GT","best":1,"bad":0},
    "per_pred_precision":{"text":"Precision / Pred","best":1,"bad":0},
    "residual_raw":{"text": "Post Residual"},
    
    "distance_score_raw":{"text": "Distance"},
    "backward_score_raw":{"text": "Backward"},
    "acceleration_score_raw":{"text": "Acceleration"},
    "rotation_score_raw":{"text": "Rotation"},
    "conflict_score_raw":{"text": "Overlapping"},
    "feasibility_score_raw":{"text": "Feasible"},
    
    }

global color_pallette 
# color_pallette = np.array([[117,158,186],   # for primary data
#                            [160,120,120],   # for baseline / old data
#                            [210,220,220],   # for other plots
#                            [220,220,210],   # for other plots
#                            [220,210,220],   # for other plots
#                            [110,120,120],   # for other plots
#                            [120,110,120],   # for other plots
#                            [120,120,110],   # for other plots
#                            [220,220,220],   # for cmap
#                            [255,255,255]    # for pane default color
#                            ])
color_pallette = np.array([[131, 121, 163], # for primary data
                           [163, 153, 116], #  for baseline / old data
                           [210,220,220],   # for other plots
                           [220,220,210],   # for other plots
                           [220,210,220],   # for other plots
                           [110,120,120],   # for other plots
                           [120,110,120],   # for other plots
                           [120,120,110],   # for other plots
                           [220,220,220],   # for cmap
                           [255,255,255]    # for pane default color
                           ])
color_pallette = random_pallette(10)
color_pallette[-2:,:] = np.array([[220,220,220],[255,255,255]])

global primary_colors
primary_colors = (np.random.rand(8,3)-0.5) * 20 + color_pallette[0][None,:]

global secondary_colors
secondary_colors = (np.random.rand(8,3)-0.5) * 20 + color_pallette[1][None,:]


# primary_colors = primary_colors[:,::-1]
# secondary_colors = secondary_colors[:,::-1]
# color_pallette = color_pallette[:,::-1]


global cmap
#cmap = lambda x: np.array([(255*(1-x),140,255*x)]).astype(np.uint8).tolist()
cmap = lambda x: color_pallette[0]*(x) + color_pallette[-2]*(1-x)

global classmap
classmap = ["sedan","midsize","van","pickup","semi","truck"]




#%% DONE

def f2a(fig):
    io_buf = io.BytesIO()
    fig.savefig(io_buf, format='raw')#, dpi=dpi)
    io_buf.seek(0)
    img_arr = np.reshape(np.frombuffer(io_buf.getvalue(), dtype=np.uint8),
                         newshape=(int(fig.bbox.bounds[3]), int(fig.bbox.bounds[2]), -1))
    io_buf.close()
    
    # RGB to BGR
    img_arr = img_arr.copy()
    temp = img_arr[:,:,0].copy()
    img_arr[:,:,0] = img_arr[:,:,2]
    img_arr[:,:,2] = temp
    return img_arr

def gen_title(results,figsize):
    name = results[0]["name"]
    iou = results[0]["iou_threshold"]
    try:
        comment = results[0]["description"]
    except:
        comment = ""
    try:
        gt_dataset = results[0]["gt"]
    except:
        gt_dataset = "No GT data"
    
    
    fig = plt.figure(figsize=(figsize[0]/scale,figsize[1]/scale))
    
    fig.text(0.01,0.8,"Collection: ",fontsize = 2500/scale)
    fig.text(0.25,0.8,"{}".format(name),fontsize = 2500/scale,color = color_pallette[0]/255)

    fig.text(0.01,0.3,"Notable changes:",fontsize = 1500/scale)
    fig.text(0.25,0.3,"{}".format(comment),fontsize = 1500/scale,wrap = True, va = "top")

    fig.text(0.01,0.45,"GT IOU:",fontsize = 1500/scale)
    fig.text(0.25,0.45,"{} ({})".format(iou,gt_dataset),fontsize = 1500/scale)

    if len(results) > 1:
        baseline_name = results[1]["name"]
        fig.text(0.01,0.6,"Baseline: ",fontsize = 1500/scale)
        fig.text(0.25,0.6,"{}".format(baseline_name),fontsize = 1500/scale,color = color_pallette[1]/255)

    return f2a(fig)

def unsup_title(results,figsize):
    fig = plt.figure(figsize=(figsize[0]/scale,figsize[1]/scale))
    fig.text(0.5,0.5,"Unsupervised Statistics",fontsize = 2500/scale, va = "center",ha = "center")
    return f2a(fig)

def sup_title(results,figsize):
    fig = plt.figure(figsize=(figsize[0]/scale,figsize[1]/scale))
    fig.text(0.5,0.5,"Supervised Metrics",fontsize = 2500/scale, va = "center",ha = "center")
    return f2a(fig)

def bar_MOT(results,figsize):
    """
    Ravel supervised MOT metrics into a bar_chart
    """
    
    include = ["idp","idr","recall","precision","mota","motp","gt_match", "pred_match"]#"true_negative_rate"]
    include = include[::-1]
    

    
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    ax = fig.add_subplot(111)
    ax.spines.top.set_visible(False)
    ax.spines.left.set_visible(True)
    ax.spines.right.set_visible(False)
    ax.yaxis.set_ticks([])
    ax.tick_params(axis='x', labelsize= 1500/scale )  
    ax.set_xlim([0,1.1])
    #ax.set_position([0,0,1,1])
    
    both_val = []
    for ridx,result in enumerate(results):
        name = []
        val = []
        for met in include:
            if met in result:
                name.append(met)
                val.append(result[met])
    
        x_pos = np.arange(len(name))
        ax.barh(x_pos,val, align='center', color = color_pallette[ridx]/255, alpha=0.3)
        both_val.append(val)
        #ax.yticks(x_pos, name)
    
    for idx,lab in enumerate(name):
        # plot labels
        x_plot = val[idx]/3 if val[idx] > 0.3 else 0.5
        plot_lab = MD[lab]["text"]
        ax.text(x_plot,idx,plot_lab,fontsize = 1000/scale) 
        
        # plot values
        postval = both_val[0][idx]
        posttext = "{:.2f}".format(postval)
        x_plot = val[idx] + 0.01

        # plot old value if available
        if len(results) > 1:
            preval = both_val[1][idx]
            x_plot = max(both_val[0][idx],both_val[1][idx]) + 0.01

            pretext = "Old: {:.2f}".format(preval)
            ax.text(x_plot,idx+0.125,pretext,fontsize = 1000/scale,color = color_pallette[1]/255)

            diff = postval - preval
            sign = "" if diff < 0 else "+"
            posttext = "New: {:.2f} ({}{:.2f})".format(postval,sign,diff)        

        ax.text(x_plot,idx-0.125,posttext,fontsize = 1000/scale,color = color_pallette[0]/255)
        
       
    
    # Title
    fig.text(0.5,0.95,"MOT Metrics",fontsize = 2000/scale,ha = "center")
    return f2a(fig)

def conf_matrix(result,figsize):    
    n_c = len(classmap)
    confusion_matrix = result[0]["confusion_matrix"].data.numpy()[:n_c,:n_c] #* 1000 
    #plot confusion matrix
    sums = np.sum(confusion_matrix,axis= 0)
    sumss = sums[:,np.newaxis] + 0.00001
    sumss = np.repeat(sumss,confusion_matrix.shape[0],1)#.transpose()
    sumss = np.transpose(sumss)
    percentages = np.round(confusion_matrix/sumss * 100)
    
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    ax = fig.add_subplot(111)
    
    colored = np.zeros([percentages.shape[0],percentages.shape[1],3])
    for i in range(colored.shape[0]):
        for j in range(colored.shape[1]):
            colored[i,j,:] = cmap(percentages[i,j]/100)
    
    colored /= 255
    
    im = ax.imshow(colored)#,cmap = "YlGn")
    
    classes =  classmap
    # We want to show all ticks...
    ax.set_xticks(np.arange(len(classes)))
    ax.set_yticks(np.arange(len(classes)))
    # ... and label them with the respective list entries
    ax.set_xticklabels(classes)
    ax.set_yticklabels(classes)
    ax.set_ylim(len(classes)-0.5, -0.5)
    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=0, ha="center",
             rotation_mode="anchor",fontsize = 1400/scale)
    plt.setp(ax.get_yticklabels(), rotation=45, va="center",
         rotation_mode="anchor",fontsize = 1400/scale)
    
    # Loop over data dimensions and create text annotations.
    for i in range(len(classes)):
        for j in range(len(classes)):
            text = ax.text(j, i, "{}".format(int(confusion_matrix[i, j])),
                       ha="center", va="bottom", color="k",fontsize = 1000/scale)
            text = ax.text(j, i, str(percentages[i, j])+"%",
                       ha="center", va="top", color="k",fontsize = 1400/scale)
    
    ax.set_ylabel("Actual",fontsize = 1800/scale)
    ax.set_xlabel("Predicted",fontsize = 1800/scale)
    
    fig.text(0.5,0.95,"Class Confusion Matrix",fontsize = 2000/scale,ha = "center")

    
    return f2a(fig)
    
def death_pie(results,figsize):
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    ax = fig.add_subplot(111)
    
    data = results[0]["cause_of_death"]
    
    labels = list(data.keys())
    vals = list(data.values())
    explode = ([0 for _ in range(len(labels))])  # only "explode" the 2nd slice (i.e. 'Hogs')
    colors = [color_pallette[i+2]/255 for i in range(len(labels))]
    
    # find "good" death type
    for lidx in range(len(labels)):
        if "FOV" in labels[lidx] or "fov" in labels[lidx]:
            good_idx = lidx
            break
    explode[good_idx] = 0.1
    colors[good_idx] = color_pallette[0]/255

    
    ax.pie(vals, explode=explode, labels=labels, autopct='%1.1f%%', colors = colors, textprops={'fontsize': 1400/scale},
            shadow=True, startangle=90, wedgeprops={"alpha":0.5})
    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    
    fig.text(0.5,0.95,"Object Cause of Death",fontsize = 2000/scale,ha = "center")
    #plt.tight_layout()


    return f2a(fig)

def state_error(results,figsize):
    state_names = ["X Position","Y Position", "Length", "Width", "Height"]
    
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    # ax = fig.add_subplot(111)
    # ax.spines.top.set_visible(False)
    # ax.spines.left.set_visible(True)
    # ax.spines.right.set_visible(False)
    
    # get data
    data = results[0]["state_error"]
    xx = np.arange(-8,8,0.05)
    data = data.transpose(1,0)
    data_pdf = []
    for i in range(data.shape[0]):
        pdf = gaussian_kde(data[i])
        data_pdf.append(pdf(xx))
    data_pdf = np.stack(data_pdf)
    
    if len(results) > 1:
        data2 = results[1]["state_error"]
        data2 = data2.transpose(1,0)
        data2_pdf = []
        for i in range(data2.shape[0]):
            pdf = gaussian_kde(data2[i])
            data2_pdf.append(pdf(xx))
        data2_pdf = np.stack(data2_pdf)
        
    max_val = np.max(data_pdf)
    min_xval = min(xx)
    
    gs = (grid_spec.GridSpec(data.shape[0],1))
    
    #creating empty list
    ax_objs = []
    for didx in range(len(data_pdf)):
        # creating new axes object and appending to ax_objs
        ax_objs.append(fig.add_subplot(gs[didx:didx+1, 0:]))
    
        # plotting the distribution
        # filling the space beneath the distribution
        if len(results) > 1:
            ax_objs[-1].fill_between(xx,data2_pdf[didx],alpha = 1,color = color_pallette[1]/255)
        
        ax_objs[-1].fill_between(xx,data_pdf[didx],alpha = 0.5,color = color_pallette[0]/255)
        
        ax_objs[-1].plot(xx,data_pdf[didx],color = "black",linewidth = 2)
        ax_objs[-1].plot(xx,data2_pdf[didx],color = color_pallette[1]/255)


    
        # setting uniform x and y lims
        ax_objs[-1].set_xlim(min(xx),max(xx))
        ax_objs[-1].set_ylim(0,max_val)
    
        # make background transparent
        rect = ax_objs[-1].patch
        rect.set_alpha(0)
        
        # remove borders, axis ticks, and labels
        ax_objs[-1].set_yticklabels([])
        ax_objs[-1].set_yticks([])
        ax_objs[-1].set_ylabel('')
        
        if didx == data.shape[0]-1:
            ax_objs[-1].tick_params(axis='x', labelsize=1000/scale,length = 500/scale )
            ax_objs[-1].set_xlabel("Error Distribution (ft)", fontsize = 1500/scale)
        else:
            ax_objs[-1].set_xticklabels([])
            ax_objs[-1].tick_params(axis='x', length = 200/scale )

        
        spines = ["top","right","left"]#,"bottom"]
        for s in spines:
            ax_objs[-1].spines[s].set_visible(False)
        ax_objs[-1].text(min_xval,max_val/2,state_names[didx],fontweight="bold",fontsize = 1500/scale,va="bottom")
        
        ax_objs[-1].axvline(x = 0, ymax = 0.8, linestyle = ":",color = (0.2,0.2,0.2))
        
        
        # get mean,MAE, stddev, max
        mean = np.mean(data[didx])
        MAE  = np.mean(np.abs(data[didx]))
        stddev = np.std(data[didx])
        maxx = np.max(np.abs(data[didx]))
        
        ax_objs[-1].text(min_xval,max_val/2,                "Mean:  {:.1f}ft".format(mean),fontsize = 1000/scale,va="top")
        ax_objs[-1].text(min_xval,max_val/2-(0.05*max_val), "MAE:   {:.1f}ft".format(MAE),fontsize = 1000/scale,va="top")
        ax_objs[-1].text(min_xval,max_val/2-(0.1*max_val),  "Stdev: {:.1f}ft".format(stddev),fontsize = 1000/scale,va="top")
        ax_objs[-1].text(min_xval,max_val/2-(0.15*max_val), "Max:   {:.1f}ft".format(maxx),fontsize = 1000/scale,va="top")
        
        if len(results) > 1:
            
            mean = np.mean(data2[didx])
            MAE  = np.mean(np.abs(data2[didx]))
            stddev = np.std(data2[didx])
            maxx = np.max(np.abs(data2[didx]))
            
            xv = min_xval + 1.7
            ax_objs[-1].text(xv,max_val/2,               "({:.1f}ft)".format(mean),fontsize = 1000/scale,va="top", color= color_pallette[1]/255)
            ax_objs[-1].text(xv,max_val/2-(0.05*max_val),"({:.1f}ft)".format(MAE),fontsize = 1000/scale,va="top", color= color_pallette[1]/255)
            ax_objs[-1].text(xv,max_val/2-(0.1*max_val), "({:.1f}ft)".format(stddev),fontsize = 1000/scale,va="top", color= color_pallette[1]/255)
            ax_objs[-1].text(xv,max_val/2-(0.15*max_val),"({:.1f}ft)".format(maxx),fontsize = 1000/scale,va="top", color= color_pallette[1]/255)

    plt.tight_layout()
    gs.update(hspace= -0.2)
    
    
    fig.text(0.5,0.95,"State Error Histograms",fontsize = 2000/scale,ha = "center")
    return f2a(fig)

def unsup_hist2(results,figsize):
    to_plot = ["distance_score_raw","backward_score_raw","acceleration_score_raw","rotation_score_raw","feasibility_score_raw"] #"conflict_score_raw",
    units = ["","","","","",""]
    title = "Unsupervised Score"
    
    xrange_clip = [[0,1],[0,1],[0,1],[0,1],[0,1],[0,1]]
    
    return unsup_hist(results,figsize,to_plot,units,xrange_clip,title,uniform_x = True)


def unsup_hist(results,
               figsize,
               to_plot = ["x_traveled_raw","avg_vx_raw","avg_ax_raw","vx_raw","ax_raw","residual_raw"],
               units = ["ft","ft/s","ft/$s^2$","ft/s","ft/$s^2$","ft"],
               xrange_clip = [[-500,2000],[0,150],[-8,8],[0,150],[-8,8],[0,100]],
               title = "Trajectory Attribute Histograms",
               uniform_x = False
               ):

    if ("residual_raw" not in results[0].keys() or sum(results[0]["residual_raw"]) == 0) and "residual_raw" in to_plot:
        to_plot = ["x_traveled_raw","avg_vx_raw","avg_ax_raw","vx_raw","ax_raw"]
    
    
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    # ax = fig.add_subplot(111)
    # ax.spines.top.set_visible(False)
    # ax.spines.left.set_visible(True)
    # ax.spines.right.set_visible(False)
    
    # get data
    data = [np.array(results[0][key]) for key in to_plot]
    data = [data[i] -0.01*np.random.rand(*data[i].shape) for i in range(len(data))]
    
    #data = np.stack(data)
    #xx = np.arange(-8,8,0.05)
    data_pdf = []
    for i in range(len(data)):
        
        pdf = gaussian_kde(data[i])
        xx = np.arange(np.min(data[i]),np.max(data[i]),1)
        xx = np.linspace(np.min(data[i]),np.max(data[i]),200)
        data_pdf.append([xx,pdf(xx)])
            
    #data_pdf = np.stack(data_pdf)
    
    if len(results) > 1:
        data2 = [np.array(results[1][key]) for key in to_plot]
        data2 = [data2[i] -0.01*np.random.rand(*data2[i].shape) for i in range(len(data2))]
        #data2 = np.stack(data2)
        data2_pdf = []
        for i in range(len(data2)):
            try:
                pdf = gaussian_kde(data2[i])
                xx = np.arange(np.min(data2[i]),np.max(data2[i]),1)
                xx = np.linspace(np.min(data[i]),np.max(data[i]),200)
                data2_pdf.append([xx,pdf(xx)])
            except:
                data2_pdf.append([[0],[0]])
        #data2_pdf = np.stack(data2_pdf)
    
    gs = (grid_spec.GridSpec(len(data),1))
    
    #creating empty list
    ax_objs = []
    for didx in range(len(data_pdf)):
        
        if to_plot[didx] not in results[0].keys():
            continue
        
        xx,dataD = data_pdf[didx]
        # creating new axes object and appending to ax_objs
        ax_objs.append(fig.add_subplot(gs[didx:didx+1, 0:]))
        
        max_val = np.max(dataD)
        xr = [min(xx),max(xx)]

        # plotting the distribution
        # filling the space beneath the distribution
        if len(results) > 1 and to_plot[didx] in results[1].keys():
            xx2,dataD2 = data2_pdf[didx]
            ax_objs[-1].fill_between(xx2,dataD2,alpha = 1,color = color_pallette[1]/255)
            ax_objs[-1].plot(xx2,dataD2,color = color_pallette[1]/255)
            
            max_val = max(np.max(dataD),np.max(dataD2))
            xr = [min(min(xx),min(xx2)),max(max(xx),max(xx2))]
            
        ax_objs[-1].fill_between(xx,dataD,alpha = 0.5,color = color_pallette[0]/255)
        ax_objs[-1].plot(xx,dataD,color = "black",linewidth = 2)


    
        # setting uniform x and y lims
        xr[0] = max(xr[0],xrange_clip[didx][0])
        xr[1] = min(xr[1],xrange_clip[didx][1])
        
        if uniform_x:
            xr = xrange_clip[didx]
        

        ax_objs[-1].set_xlim(xr[0],xr[1])
        ax_objs[-1].set_ylim(0,max_val)
    
        # make background transparent
        rect = ax_objs[-1].patch
        rect.set_alpha(0)
        
        # remove borders, axis ticks, and labels
        ax_objs[-1].set_yticklabels([])
        ax_objs[-1].set_yticks([])
        ax_objs[-1].set_ylabel('')
        
        #if didx == data.shape[0]-1:
        ax_objs[-1].tick_params(axis='x', labelsize=1000/scale,length = 500/scale )
        ax_objs[-1].set_xlabel("{}".format(units[didx]), fontsize = 1500/scale)
        # else:
        #     ax_objs[-1].set_xticklabels([])
        #     ax_objs[-1].tick_params(axis='x', length = 200/scale )

        
        spines = ["top","right","left"]#,"bottom"]
        for s in spines:
            ax_objs[-1].spines[s].set_visible(False)
        ax_objs[-1].text(xr[0],max_val/2,MD[to_plot[didx]]["text"],fontweight="bold",fontsize = 1500/scale,va="bottom")
        #ax_objs[-1].axvline(x = 0, ymax = 0.8, linestyle = ":",color = (0.2,0.2,0.2))
        
        
        # get mean,MAE, stddev, max
        mean = np.mean(data[didx])
        MA  = np.mean(np.abs(data[didx]))
        stddev = np.std(data[didx])
        maxx = np.max(np.abs(data[didx]))
        
        xv = xr[0] #+  0.2*(xr[1] - xr[0])
        ax_objs[-1].text(xv,max_val/2,               "Mean:     {:.2f}{}".format(mean,units[didx]),fontsize = 1000/scale,va="top")
        ax_objs[-1].text(xv,max_val/2-(0.05*max_val),"Mean Abs: {:.2f}{}".format(MA,units[didx]),fontsize = 1000/scale,va="top")
        ax_objs[-1].text(xv,max_val/2-(0.1*max_val), "Stdev:    {:.2f}{}".format(stddev,units[didx]),fontsize = 1000/scale,va="top")
        ax_objs[-1].text(xv,max_val/2-(0.15*max_val),"Max:      {:.2f}{}".format(maxx,units[didx]),fontsize = 1000/scale,va="top")
        
        if len(results) > 1:
            mean = np.mean(data2[didx])
            MA  = np.mean(np.abs(data2[didx]))
            stddev = np.std(data2[didx])
            maxx = np.max(np.abs(data2[didx]))
            
            xv = xr[0] + 0.16*(xr[1] - xr[0])
            # may need a black border
            
            ax_objs[-1].text(xv,max_val/2,     " ({:.2f}{})".format(mean,units[didx]),fontsize = 1000/scale,va="top",ha="left", color=color_pallette[1]/255,bbox=dict(facecolor="white",edgecolor="white",alpha=0.7))
            ax_objs[-1].text(xv,max_val/2-(0.05*max_val)," ({:.2f}{})".format(MA,units[didx]),fontsize = 1000/scale,va="top",ha="left", color= color_pallette[1]/255,bbox=dict(facecolor="white",edgecolor="white",alpha=0.7))
            ax_objs[-1].text(xv,max_val/2-(0.1*max_val), " ({:.2f}{})".format(stddev,units[didx]),fontsize = 1000/scale,va="top",ha="left", color= color_pallette[1]/255,bbox=dict(facecolor="white",edgecolor="white",alpha=0.7))
            ax_objs[-1].text(xv,max_val/2-(0.15*max_val)," ({:.2f}{})".format(maxx,units[didx]),fontsize = 1000/scale,va="top",ha="left", color= color_pallette[1]/255,bbox=dict(facecolor="white",edgecolor="white",alpha=0.7))

    plt.tight_layout()
    gs.update(hspace= -0.2)
    
    
    fig.text(0.5,0.95,title,fontsize = 2000/scale,ha = "center")
    return f2a(fig)

def chart_MOT(results,figsize):
    c_scale = 50000
    
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    ax = fig.add_subplot(111)
    ax.set_position([0,0,1,1])
    ax.axis("off")

    # assemble list of metrics with ideal value (or -1 if none), new value, [old value]
    to_list = ["mostly_tracked%","mostly_lost%","num_switches_per_gt","num_fragmentations_per_gt","pred_avg_matches","gt_avg_matches"]
    coll_names = [result["name"] for result in results]
    cell_text = []
    
    
    data = np.zeros([len(to_list),len(coll_names)+1])
    
    print(results[0].keys())
    
    for qidx,quant in enumerate(to_list):
        row_text = []
        for ridx,result in enumerate(results):
            if quant in result.keys():
                data[qidx,ridx] = result[quant]
            else:
                data[qidx,ridx] = np.nan
        data[qidx,-1] = MD[quant]["best"]
    
    # normalize data by max data[:,0]
    data_norm = data / np.max(data[:,:2],axis = 1)[:,None]
    data_norm = data_norm.clip(0.001)
    
    w2h = figsize[0] / figsize[1]
    
    ns = len(to_list)
    nr = np.ceil(np.sqrt(ns/w2h)) 
    nc = np.ceil(  ns/nr)
    
    assert nc*nr >=ns, "Not enough cells for data"
    
    for idx in range(ns):
        for didx in range(len(results)-1,-1,-1):
            ax.scatter(idx//nr,idx%nr,s=data_norm[idx,didx]*c_scale,color = [color_pallette[didx]/255], alpha = 0.6)
        
        
        tshift = 0.2
        ax.text(idx//nr,idx%nr+tshift,MD[to_list[idx]]["text"],va="bottom",ha="center", fontsize = 1500/scale)
        txt = "{:.2f} ".format(data[idx,0])
        ax.text(idx//nr,idx%nr+tshift,txt,va="top",ha="right", fontsize = 1000/scale)
    
        if len(results) > 1:
            txt = " $\leftarrow$ ({:.2f})".format(data[idx,1])
            ax.text(idx//nr,idx%nr+tshift,txt,va="top",ha="left", fontsize = 1000/scale)
           
        best = data_norm[idx,2]
        if not np.isnan(best):
            # if best == 0:
            #     best += 0.0001
            ax.scatter(idx//nr,idx%nr,s=best*c_scale,facecolors='none', edgecolors=[0,0,0],linewidth = 2)
    
    ax.set_xlim([-0.5,nc-0.5])
    ax.set_ylim([-0.5,nr-0.3])
    
    return f2a(fig)

def chart_unsup(results,figsize):
    
    c_scale = 50000
    
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    ax = fig.add_subplot(111)
    ax.set_position([0,0,1,1])
    ax.axis("off")

    # assemble list of metrics with ideal value (or -1 if none), new value, [old value]
    to_list = ["traj_count","bps","x_variation","y_variation","duration_avg","x_traveled_avg","percent_backwards","overlaps_per_object"]
    coll_names = [result["name"] for result in results]
    cell_text = []
    
    
    data = np.zeros([len(to_list),len(coll_names)+1])
    
    
    
    for qidx,quant in enumerate(to_list):
        row_text = []
        for ridx,result in enumerate(results):
            if quant in result.keys():
                data[qidx,ridx] = result[quant]
            else:
                data[qidx,ridx] = 0
        data[qidx,-1] = MD[quant]["best"]
    
    # normalize data by max data[:,0]
    data += 0.0000001
    data_norm = data / np.max(data[:,:2],axis = 1)[:,None]
    data_norm = data_norm.clip(0.001)
    
    w2h = figsize[0] / figsize[1]
    
    ns = len(to_list)
    nr = np.ceil(np.sqrt(ns/w2h)) 
    nc = np.ceil(  ns/nr)
    
    assert nc*nr >=ns, "Not enough cells for data"
    
    for idx in range(ns):
        for didx in range(len(results)-1,-1,-1):
            ax.scatter(idx//nr,idx%nr,s=data_norm[idx,didx]*c_scale,color = [color_pallette[didx]/255], alpha = 0.6)
        
        
        tshift = 0.2
        ax.text(idx//nr,idx%nr+tshift,MD[to_list[idx]]["text"],va="bottom",ha="center", fontsize = 1500/scale)
        txt = "{:.2f} ".format(data[idx,0])
        ax.text(idx//nr,idx%nr+tshift,txt,va="top",ha="right", fontsize = 1000/scale)
    
        if len(results) > 1:
            txt = " $\leftarrow$ ({:.2f})".format(data[idx,1])
            ax.text(idx//nr,idx%nr+tshift,txt,va="top",ha="left", fontsize = 1000/scale)
           
        best = data_norm[idx,2]
        if not np.isnan(best):
            # if best == 0:
            #     best += 0.0001
            ax.scatter(idx//nr,idx%nr,s=best*c_scale,facecolors='none', edgecolors=[0,0,0],linewidth = 2)
    
    ax.set_xlim([-0.5,nc-0.5])
    ax.set_ylim([-0.5,nr-0.3])
    
    return f2a(fig)

def history(results,figsize):
    """
    Collect all results within the given directory (same directory as results comes from)
    For each, determine whether calculated on same GT
    If so, get date (x_axis)
    Get aggregate score (y-axis)
    Get framerate (dot size)
    
    Scatter each (hollow if raw and solid if post-processed)
    Plot tie-lines from raw collectio to all post-processed collections
    
    Repeat plot of baseline and primary collections in correct colors
    """
    secondary_scale_power = 1            # adjust variation
    PSCALE = 3000                       # adjust avg size

    primary_metric = "Aggregate Score" # must be in spider
    secondary_metric = "Realtime" # adjusts size, must be in spider
    tertiary_metric = "recall" # adjusts transparency
    x_metric = "gen_time"
    
    names = []
    primaries = []
    secondaries = []
    tertiaries = []
    raw_names = []
    datetimes = []
    postprocessed = []
        
    for coll in os.listdir(results_dir):
        
        
        with open(os.path.join(results_dir,coll),"rb") as f:
            hist_result = pickle.load(f)
        hist_gt_coll = hist_result["gt"]
        if hist_gt_coll == results[0]["gt"]: # only keep results on same GT
            try:
                hist_result = agg_score(hist_result)
            except:
                continue
        
            primaries.append(hist_result[primary_metric])
            secondaries.append(hist_result["spider"][secondary_metric])  
            tertiaries.append(hist_result[tertiary_metric])  
            names.append(coll.split(".")[0])
            raw_names.append(coll.split(".")[0].split("--")[0])
            datetimes.append(hist_result[x_metric])
            postprocessed.append(hist_result["postprocessed"])
    
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    ax = fig.add_subplot(111)
    
    rn = max(primaries) - min(primaries)
    
    best_idx = np.argmax(np.array(primaries))
    for idx in range(len(names)):
        #print(names[idx])
        x = datetimes[idx]
        y = primaries[idx]
        s = PSCALE * np.power(secondaries[idx],secondary_scale_power)
        color =  color_pallette[-2]/255
        fc= color if not postprocessed[idx] else "none"
        plt.scatter(x,y,s,facecolor = fc,alpha = tertiaries[idx])
        if postprocessed[idx]:
            plt.scatter(x,y,s,linewidth = 2,edgecolor =  (.3,.3,.3),facecolor = fc)

        if idx == best_idx or names[idx] == results[0]["name"] or names[idx] == results[1]["name"]:
            text_name = (names[idx].split("_")[-1] if postprocessed[idx] else raw_names[idx]) + " ({:.2f})".format(primaries[idx])
            plt.text(x,y+0*(rn),text_name,va = "center", ha = "center",fontsize = 600/scale,rotation = 0)
    
    # plot ties
    if False:
        for i in range(len(names)):
            for j in range(len(names)):
                if postprocessed[i] and not postprocessed[j] and raw_names[i] == raw_names[j]: 
                    x = [datetimes[i],datetimes[j]]
                    y = [primaries[i],primaries[j]]
                    plt.plot(x,y,linestyle = ":", color = (.3,.3,.3),linewidth = 1)
    
    if x_metric == "gen_time":
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
        ax.xaxis.set_major_locator(mdates.DayLocator(interval=3))
    
    ax.spines["top"].set_visible(False)
    #ax.spines["left"].set_visible(False)
    ax.spines["right"].set_visible(False)

    #ax.set_ylabel("Aggregate Score",fontsize = 1500/scale)
    ax.set_xlabel(x_metric,fontsize = 1500/scale)
    ax.set_ylabel(primary_metric,fontsize = 1500/scale)

    #ax.set_yticklabels([])
    #ax.set_yticks([])
    ax.tick_params(axis='both', labelsize=1000/scale, length = 10,labelrotation = 45 )
    ax.set_ylim([min(primaries)-0.125,max(primaries)+0.125])
    
    
    # replot primary
    prim = results[0][primary_metric]
    second = results[0]["spider"][secondary_metric]
    date = results[0][x_metric]
    post = results[0]["postprocessed"]
    s = PSCALE * np.power(second,secondary_scale_power)
    fc = color_pallette[0]/255 #if post else "none"
    lw =  4
    edge_color = color_pallette[0] / 255
    plt.scatter(date,prim,s,linewidth = lw,edgecolor =  edge_color,facecolor = fc)
    
    # replot  secondary
    if len(results) > 1:
        prim = results[1][primary_metric]
        second = results[1]["spider"][secondary_metric]
        date = results[1][x_metric]
        post = results[1]["postprocessed"]
        s = PSCALE * np.power(second,secondary_scale_power)
        fc = color_pallette[1]/255 #if post else "none"
        lw =  4
        edge_color = color_pallette[1] / 255
        plt.scatter(date,prim,s,linewidth = lw,edgecolor =  edge_color,facecolor = fc)
    
    fig.subplots_adjust(bottom=0.25)
    
    # write out best results so far
    best_idx = np.argmax(np.array(primaries))
    best_coll= names[best_idx]
    best_text= "Current Best Result: {} ({:.1f})".format(best_coll,primaries[best_idx])
    fig.text(0.02,0.98,best_text, va = "top", fontsize = 1000/scale)
    
    primaries = [primaries[idx] * int(not postprocessed[idx]) for idx in range(len(primaries))]
    best_idx = np.argmax(np.array(primaries))
    best_coll= names[best_idx]
    best_text= "Current Best Raw Result: {} ({:.1f})".format(best_coll,primaries[best_idx])
    fig.text(0.02,0.94,best_text, va = "top", fontsize = 1000/scale)
    
    return f2a(fig)


def iou_scatter(results,figsize):

    
    
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    ax = fig.add_subplot(111)
    
    ridx =0
    x_val = results[ridx]["match_overlap"]["conf"]
    y_val = results[ridx]["match_overlap"]["iou"]
    # plot_windows = np.arange(0,1,0.025)
    # for i in range(len(plot_windows)-1):
    #     imin = plot_windows[i]
    #     imax = plot_windows[i+1]
    #     select_idx = np.argwhere(np.bitwise_and(x_val > imin, x_val < imax ).astype(int))
        
    #     x_select = np.array(x_val)[select_idx]
    #     y_select = np.array(y_val)[select_idx]
    #     ax.scatter(x_select,y_select,alpha = 1/len(select_idx)**0.5,color = color_pallette[ridx]/255)
    
    ax.scatter(x_val,y_val,alpha = 0.02,color =color_pallette[ridx]/255, linewidths = 0)
    
    ax.set_ylim([0,1])
    ax.set_ylabel("IOU",fontsize = 1500/scale)
    ax.set_xlim([0,1])
    ax.set_xlabel("Detection Confidence",fontsize = 1500/scale)
    ax.tick_params(axis='both', labelsize=1000/scale, length = 10)
    fig.text(0.5,0.94,"IOU v Confidence Correlation", va = "top", fontsize = 1500/scale)
    
   
    
    reg = LinearRegression().fit(np.array(x_val).reshape(-1,1),y_val)
    y_reg = reg.predict(np.array([[0],[1]]))
    score = reg.score(np.array(x_val).reshape(-1,1),y_val)
    
    plt.plot([0,1],y_reg,color = (1,0,0), linewidth = 2)
    plt.text( 0.25, reg.predict(np.array([[0.25]])), "R$^2$ = {:.3f}".format(score), va = "bottom",fontsize = 1500/scale)
    
    
    
    ax.spines.right.set_visible(False)
    ax.spines.top.set_visible(False)

    
    return f2a(fig)

def sup_hist(results,figsize):
    
    """
    These quantities should be in the range [0,1]
    """
    
    to_plot = ["per_gt_recall","per_pred_precision"]
    units = ["",""]
    xrange_clip = [[0,1],[0,1],[-200,200]]
    
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    # ax = fig.add_subplot(111)
    # ax.spines.top.set_visible(False)
    # ax.spines.left.set_visible(True)
    # ax.spines.right.set_visible(False)
    
    # get data
    data = [results[0][key] for key in to_plot]
    xx = np.arange(0,1,0.01)
    data_pdf = []
    for i in range(len(data)):
        pdf = gaussian_kde(data[i])
        data_pdf.append([xx,pdf(xx)])
    
    if len(results) > 1:
        data2 = [results[1][key] for key in to_plot]
        #data2 = np.stack(data2)
        data2_pdf = []
        for i in range(len(data2)):
            pdf = gaussian_kde(data2[i])
            data2_pdf.append([xx,pdf(xx)])
    
    gs = (grid_spec.GridSpec(len(data),1))
    
    #creating empty list
    ax_objs = []
    for didx in range(len(data_pdf)):
        xx,dataD = data_pdf[didx]
        # creating new axes object and appending to ax_objs
        ax_objs.append(fig.add_subplot(gs[didx:didx+1, 0:]))
        
        max_val = np.max(dataD)
        xr = [min(xx),max(xx)]

        # plotting the distribution
        # filling the space beneath the distribution
        if len(results) > 1:
            xx2,dataD2 = data2_pdf[didx]
            ax_objs[-1].fill_between(xx2,dataD2,alpha = 0.6,color = color_pallette[1]/255)
            ax_objs[-1].plot(xx2,dataD2,color = color_pallette[1]/255)
            
            max_val = max(np.max(dataD),np.max(dataD2))
            xr = [min(min(xx),min(xx2)),max(max(xx),max(xx2))]
            
        ax_objs[-1].fill_between(xx,dataD,alpha = 0.5,color = color_pallette[0]/255)
        ax_objs[-1].plot(xx,dataD,color = "black",linewidth = 2)


    
        # setting uniform x and y lims
        xr[0] = max(xr[0],xrange_clip[didx][0])
        xr[1] = min(xr[1],xrange_clip[didx][1])

        ax_objs[-1].set_xlim(xr[0],xr[1])
        ax_objs[-1].set_ylim(0,max_val)
    
        # make background transparent
        rect = ax_objs[-1].patch
        rect.set_alpha(0)
        
        # remove borders, axis ticks, and labels
        ax_objs[-1].set_yticklabels([])
        ax_objs[-1].set_yticks([])
        ax_objs[-1].set_ylabel('')
        
        #if didx == data.shape[0]-1:
        ax_objs[-1].tick_params(axis='x', labelsize=1000/scale,length = 500/scale )
        ax_objs[-1].set_xlabel("{}".format(units[didx]), fontsize = 1500/scale)
        # else:
        #     ax_objs[-1].set_xticklabels([])
        #     ax_objs[-1].tick_params(axis='x', length = 200/scale )

        
        spines = ["top","right","left"]#,"bottom"]
        for s in spines:
            ax_objs[-1].spines[s].set_visible(False)
        yv = max_val
        ax_objs[-1].text(xr[0],yv,MD[to_plot[didx]]["text"],fontweight="bold",fontsize = 1500/scale,va="bottom")
        #ax_objs[-1].axvline(x = 0, ymax = 0.8, linestyle = ":",color = (0.2,0.2,0.2))
        
        
        # # get mean,MAE, stddev, max
        mean = np.mean(data[didx])
        # MA  = np.mean(np.abs(data[didx]))
        # stddev = np.std(data[didx])
        # maxx = np.max(np.abs(data[didx]))
        xv = xr[0] #+  0.2*(xr[1] - xr[0])
        ax_objs[-1].text(xv,yv,               "Mean:     {:.2f}{}".format(mean,units[didx]),fontsize = 1000/scale,va="top")
        # ax_objs[-1].text(xv,max_val/2-(0.05*max_val),"Mean Abs: {:.1f}{}".format(MA,units[didx]),fontsize = 1000/scale,va="top")
        # ax_objs[-1].text(xv,max_val/2-(0.1*max_val), "Stdev:    {:.1f}{}".format(stddev,units[didx]),fontsize = 1000/scale,va="top")
        # ax_objs[-1].text(xv,max_val/2-(0.15*max_val),"Max:      {:.1f}{}".format(maxx,units[didx]),fontsize = 1000/scale,va="top")
        
        if len(results) > 1:
             mean = np.mean(data2[didx])
        #     MA  = np.mean(np.abs(data2[didx]))
        #     stddev = np.std(data2[didx])
        #     maxx = np.max(np.abs(data2[didx]))
            
             xv = xr[0] + 0.16*(xr[1] - xr[0])
        #     # may need a black border
            
             ax_objs[-1].text(xv,yv,     " ({:.2f}{})".format(mean,units[didx]),fontsize = 1000/scale,va="top",ha="left", color=color_pallette[1]/255,bbox=dict(facecolor="white",edgecolor="white",alpha=0.7))
        #     ax_objs[-1].text(xv,max_val/2-(0.05*max_val)," ({:.1f}{})".format(MA,units[didx]),fontsize = 1000/scale,va="top",ha="left", color= color_pallette[1]/255,bbox=dict(facecolor="white",edgecolor="white",alpha=0.7))
        #     ax_objs[-1].text(xv,max_val/2-(0.1*max_val), " ({:.1f}{})".format(stddev,units[didx]),fontsize = 1000/scale,va="top",ha="left", color= color_pallette[1]/255,bbox=dict(facecolor="white",edgecolor="white",alpha=0.7))
        #     ax_objs[-1].text(xv,max_val/2-(0.15*max_val)," ({:.1f}{})".format(maxx,units[didx]),fontsize = 1000/scale,va="top",ha="left", color= color_pallette[1]/255,bbox=dict(facecolor="white",edgecolor="white",alpha=0.7))

    plt.tight_layout()
    #gs.update(hspace= -0.2)
    
    
    #fig.text(0.5,0.95,"Trajectory Attribute Histograms",fontsize = 2000/scale,ha = "center")
    return f2a(fig)
    
def gen_spiderplot(results,figsize):
    
    results[0] = agg_score(results[0])
    
    
    fig = plt.figure(figsize =(figsize[0]/scale,figsize[1]/scale))
    ax = fig.add_subplot(111,projection="polar")
    ax.patch.set_facecolor((0.95,0.95,0.95))
    # Bars are sorted by the cumulative track length
    df_keys = list(results[0]["spider"].keys())
    total1 = results[0]["Aggregate Score"] 
    #df_keys.remove("Total")
    
    df_sorted = [results[0]["spider"][key] + 0.01 for key in df_keys]
    
    if len(results) > 1:
        results[1] = agg_score(results[1])
        df2_sorted = [results[1]["spider"][key] + 0.01 for key in df_keys]
        total2 = results[1]["Aggregate Score"]

    # Values for the x axis
    ANGLES = np.linspace(0.05, 2 * np.pi - 0.05, len(df_sorted), endpoint=False)
    
    # Cumulative length
    LENGTHS = df_sorted

    
    ax.set_theta_offset(0* np.pi / 2)
    ax.set_ylim(-0.2,1)
    
    # Add geometries to the plot -------------------------------------
    # See the zorder to manipulate which geometries are on top
    
    # Add bars to represent the cumulative track lengths
    if len(results) > 1:
        ax.bar(ANGLES+ 0.15*(2*np.pi/len(df_sorted)), df2_sorted, color=secondary_colors/255, alpha=0.6, width=0.52, zorder=10)
    ax.bar(ANGLES, LENGTHS, color=primary_colors/255, alpha=0.7, width=0.52, zorder=10)
    

    
    # Add dashed vertical lines. These are just for nicely dividing things
    ax.vlines(ANGLES+ 0.5*(2*np.pi/len(df_sorted)), 0, 1, color=[0,0,0], ls=(0, (4, 4)), zorder=11)
    
    # Add dots to represent the mean gain
    #ax.scatter(ANGLES, df_sorted, s=60, color=[0,0,0], zorder=11)
    
    # Remove unnecesary guides ---------------------------------------

    # Remove lines for polar axis (x)
    #ax.xaxis.grid(False)
    
    # Put grid lines for radial axis 
    ax.set_yticklabels([])
    y_ticks = [0.2,0.4,0.6,0.8,1]
    ax.set_yticks(y_ticks)
    
    # Add custom annotations -----------------------------------------
    # The following represent the heights in the values of the y axis
    # for tval in y_ticks:
    #     ax.text(np.pi / 2, tval, "{:.1f}".format(tval), ha="center", size=1000/scale)
    
    # Remove spines
    ax.spines["start"].set_color("none")
    ax.spines["polar"].set_color("none")
    
    
    # Adjust padding of the x axis labels ----------------------------
    # This is going to add extra space around the labels for the 
    # ticks of the x axis.
    XTICKS = ax.xaxis.get_major_ticks()
    for tick in XTICKS:
        tick.set_pad(20)

    # Set the labels
    ax.set_xticks(ANGLES)# + 0.5*(2*np.pi/len(df_sorted)))
    ax.set_xticklabels(df_keys, size=1500/scale)

    
    # annotate each score component
    for idx in range(len(df_keys)):
        ax.text(ANGLES[idx],0.5,"{:.2f}".format(df_sorted[idx]),size = 1500/scale,zorder = 100)

    fig.subplots_adjust(top=0.8)
    ly = 0.83
    fig.text(0.03,ly,"{:.1f}".format(total1),fontsize = 7000/scale, va = "bottom", ha = "left",color = color_pallette[0]/255)
    fig.text(0.03,ly," Score",fontsize = 2000/scale, va = "top", ha = "left")

    if len(results) > 1:
        fig.text(0.97,ly,"{:.1f}".format(total2),fontsize = 7000/scale, va = "bottom", ha = "right",color = color_pallette[1]/255)
        fig.text(0.97,ly," Baseline score",fontsize = 2000/scale, va = "top", ha = "right")
        
    weights = [" {}:{}".format(key,results[0]["score_weighting"][key]) for key in results[0]["score_weighting"].keys()]
    sum_weights = sum([results[0]["score_weighting"][key] for key in results[0]["score_weighting"].keys()])
    
    fig.text(0.01,0.01,"Score weights (Total score = {}):   {}".format(sum_weights,weights), va = "bottom", fontsize = 1000/scale, wrap = True)
    return f2a(fig)    

def gen_pane(results = [],
             size = [2160,3840],
             pane_layout = None,
             pane_functions = None,
             close = 0
             ):
    
    """
    Generates a dashboard layout with each of the specified panes filled by the 
    corresponding matplotlib plot function
    results - results dictionary generated by evaluate.py and loaded from pickle file
    pane_layout - iterable of item size 4 with grid x, grid y, width, height of each pane
    pane_functions - function used to create matplotlib (or other) figure to fill said pane (takes size as input)
    color_pallette - np.array.tolist() of size n,3
    color_spectrum - matplotlib colormap, for functions requiring spectrum - based color
    
    Importantly, the size for each plot is computed flexibly based on pane sizes and passed to the corresponding generation function,
    creating a tight layout - nice!
    """
    
    pad = 20
    shadow_pad = 18
    
    # TODO make pallette color [-1]
    dashboard = np.zeros([size[0],size[1],3]).astype(np.uint8) + 220
    
    # create placeholder panes
    pane_size = []
    pane_loc = []
    for pane in pane_layout:
        color = color_pallette[-1].tolist()
        pane_coords = (int(pane[0]*size[0]/9 + pad),int(pane[1]*size[1]/16) + pad), (int((pane[0] + pane[2])*size[0]/9 - pad),int((pane[1] + pane[3])*size[1]/16 - pad))
        shadow_coords = (int(pane[0]*size[0]/9 + pad),int(pane[1]*size[1]/16) + pad), (int((pane[0] + pane[2])*size[0]/9 - shadow_pad),int((pane[1] + pane[3])*size[1]/16 - shadow_pad))

        pane_size.append([pane_coords[1][0] - pane_coords[0][0], pane_coords[1][1] - pane_coords[0][1]])
        pane_loc.append(pane_coords)
        
        dashboard = cv2.rectangle(dashboard,shadow_coords[0],shadow_coords[1],(190,190,190),-1)
        dashboard = cv2.rectangle(dashboard,pane_coords[0],pane_coords[1],color,-1)
    
    for idx,fn in enumerate(pane_functions):
        pane_coords = pane_loc[idx]
        pane_im = fn(results,pane_size[idx])
        if pane_im is not None:
            #pane_im = cv2.imload("temp.png")
            pane_im = cv2.resize(pane_im,pane_size[idx])
            dashboard[pane_coords[0][1]:pane_coords[1][1],pane_coords[0][0]:pane_coords[1][0],:] = pane_im[:,:,:3]
        
    
    
    # display image
    dashboard = cv2.resize(dashboard,(int(size[1]*0.95),int(size[0]*0.95)))
    cv2.imshow("frame",dashboard)
    cv2.waitKey(close)
    cv2.destroyAllWindows()
    
    
    
    now = datetime.now()
    now = now.strftime("%Y-%m-%d_%H-%M-%S")
    f_name = "./push_images/{}.png".format(now)
    cv2.imwrite(f_name,dashboard)
    
    #now = now.strftime("%Y-%m-%d_%H-%M-%S")
    url = 'http://viz-dev.isis.vanderbilt.edu:5991/upload?type=track_eval'
    files = {'upload_file': open(f_name,'rb')}
    ret = requests.post(url, files=files)
    print(ret)
    if ret.status_code == 200:
        print('Uploaded!')

    
def dummy(a,b):
    pass



def lrk(results):
    # list result keys
    keylist = list(results[0].keys())
    keylist.sort()
    [print(key) for key in keylist ]
    
    
def agg_score(result):
    """
    Appends spider with agg score quantities to result
    """
    if False: # Old Score formula
    
        spider = {}
        spider["MOTA"]              = result["mota"]
        spider["Realtime"]          = result["bps"] / 30
        spider["X Error"]           = max(0, 1 - result["MAE_x"] / 5.0  )
        spider["% to Extents"]      = (  result["cause_of_death"]["Exit FOV"] + result["cause_of_death"]["Active at End"] )    /result["n_pred"]
        spider["Trajectory Length"] = min(1,result["x_traveled_avg"] /  (sum(result["gt_x_traveled"])/len(result["gt_x_traveled"])))
        spider["Avg Acceleration"]  = max(0, 1 - np.mean(np.abs(result["avg_ax_raw"])) / (5))
        spider["Classification"]    = torch.sum(torch.diag(result["confusion_matrix"])) /torch.sum(result["confusion_matrix"])
        spider["Avg GT cover"]      = sum(result["per_gt_recall"])/len(result["per_gt_recall"])
        spider["Avg Pred cover"]    = sum(result["per_pred_precision"])/len(result["per_pred_precision"])
        
        score_weighting = {
            "MOTA":2,
            "Realtime":2,
            "% to Extents":0.5,
            "X Error":1,
            "Trajectory Length":1,
            "Avg Acceleration":1,
            "Classification":0.5,
            "Avg GT cover":2,
            "Avg Pred cover":1
            }
        
    else:
        
        spider = {}
        #spider["MOTA"]              = result["mota"]
        spider["Realtime"]          = result["bps"] / 30
        #spider["X Error"]           = max(0, 1 - result["MAE_x"] / 5.0  )
        spider["Soft Feasibility"]  = result["feasibility_score_avg"]
        spider["Hard Feasibility"]  = len(result["all_feasible_id"]) / len(result["all_feasible_id"] + result["any_infeasible_id"])
        #spider["Classification"]    = torch.sum(torch.diag(result["confusion_matrix"])) /torch.sum(result["confusion_matrix"])
        #spider["Avg GT cover"]      = sum(result["per_gt_recall"])/len(result["per_gt_recall"])
        #spider["Avg Pred cover"]    = sum(result["per_pred_precision"])/len(result["per_pred_precision"])
        
        score_weighting = {
            "MOTA":2,
            "Realtime":2,
            "X Error":1,
            "Soft Feasibility":2,
            "Hard Feasibility":0.5,
            "Classification":0.5,
            "Avg GT cover":2,
            "Avg Pred cover":1
            }
    
    result["Aggregate Score"] = sum([score_weighting[key] * spider[key] for key in spider.keys()])
    result["spider"] = spider
    result["score_weighting"] = score_weighting
    
    
    
    
    
    
    
    
    return result

#%% TO BE IMPLEMENTED
    
def main(mode = "latest v latest", close = 0):
    print("Generating Results Dashboard...")
    
    if mode == "latest v latest": # atest raw and latest post
    
        latest_raw_path = None
        latest_raw_time = None
        latest_post_path = None
        latest_post_time = None
        directory = "./data/eval_results_unsupervised"
        for result_path in os.listdir(directory):
            path = os.path.join(directory,result_path)
            with open(path,"rb") as f:
                result = pickle.load(f)
                
                if not result["postprocessed"] and (latest_raw_time is None or result["gen_time"] > latest_raw_time):
                    latest_raw_time = result["gen_time"]
                    latest_raw_path = path
                
                elif result["postprocessed"]  and (latest_post_time is None or result["gen_time"] > latest_post_time):
                    latest_post_time = result["gen_time"]
                    latest_post_path = path
        results = [latest_raw_path,latest_raw_path]
    
    elif mode == "latest v best": # latest and best 

        latest_post_path = None
        latest_post_time = None
        best_path = None
        best_score = None
        directory = "./data/eval_results"
        for result_path in os.listdir(directory):
            path = os.path.join(directory,result_path)
            with open(path,"rb") as f:
                result = pickle.load(f)
                result = agg_score(result)
                if best_score is None or result["Aggregate Score"] > best_score:
                    best_score = result["Aggregate Score"]
                    best_path = path
                
                elif  (latest_post_time is None or result["gen_time"] > latest_post_time):
                    latest_post_time = result["gen_time"]
                    latest_post_path = path
        results = [latest_post_path,best_path]
    
    elif mode == "best v best": # best post and best raw
        best_post_path = None
        best_post_score = None
        best_raw_path = None
        best_raw_score = None
        directory = "./data/eval_results"
        for result_path in os.listdir(directory):
            path = os.path.join(directory,result_path)
            with open(path,"rb") as f:
                result = pickle.load(f)
                result = agg_score(result)
                if not result["postprocessed"] and (best_raw_score is None or result["Aggregate Score"] > best_raw_score):
                    best_raw_score = result["Aggregate Score"]
                    best_raw_path = path
                
                elif  (best_post_score is None or result["Aggregate Score"] > best_post_score):
                    best_post_score = result["Aggregate Score"]
                    best_post_path = path
        results = [best_post_path,best_raw_path]
    
    else:
        results = [
            "/home/derek/Documents/i24/trajectory-eval-toolkit/data/eval_results/unwieldy_markhor--RAW_GT2__escalates.cpkl",
            "/home/derek/Documents/i24/trajectory-eval-toolkit/data/eval_results/unwieldy_markhor--RAW_GT2.cpkl",
            #"/home/derek/Documents/i24/trajectory-eval-toolkit/data/eval_results/sanctimonious_beluga--RAW_GT1__administers.cpkl",
            #"/home/derek/Documents/i24/trajectory-eval-toolkit/data/eval_results/sympathetic_cnidarian--RAW_GT1__juxtaposes.cpkl"
            ]
    #results.reverse()
    
    

    for i in range(len(results)):
        
        with open(results[i],"rb") as f:
            results[i] = pickle.load(f)

    # pane = origin x, origin y, width , height
    panes = np.array([[0,0,4,1], # Title   # 
                      [0,1,4,3], # Spider
                      [0,4,4,5], # Unsup chart
                      
                      [4,0,4,6], # Unsupervised Histograms 
                      [4,6,4,3], # Death Pie
                      
                      [8,0,4,6],     # Unsupervised Histograms 2 (feasibility)    #
                      [8,6,4,3],     # Class info

                     
                      [12,0,4,3], # State error                #  
                      [12,3,4,3], # MOT chart
                      [12,6,4,3], # Additional Hover Info
                      ])
    
    pane_functions = [gen_title,
                      gen_spiderplot,
                      chart_unsup,
                      unsup_hist,
                      death_pie,
                      unsup_hist2, # feasibility
                      dummy, # eventually class dist
                      dummy,# eventually speed v time
                      dummy,#eventually throughput v time
                      dummy,# eventually density v time 
                      ]
    
    
    gen_pane(results = results,
             pane_layout = panes,
             pane_functions= pane_functions,
             close = close
             )
    
    
if __name__ == "__main__":
    main(mode = "latest v latest")
    #main(mode = "manual")
   